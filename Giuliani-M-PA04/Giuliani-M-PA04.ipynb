{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Assignment 4</h1>\n",
    "\n",
    "Lawrence Technological University<br>\n",
    "Department of Math and Computer Science<br>\n",
    "MCS 5623 Machine Learning<br>\n",
    "Assignment # 04<br>\n",
    "Michael Giuliani<br>\n",
    "10/27/2024<br>\n",
    "\n",
    "Credit Card Default Prediction (Team assignment maximum of two students) <br>\n",
    "\n",
    "The aim here is to predict whether a client will default or not. The data and information about attributes are available at<br>\n",
    "    https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients <br>\n",
    "    Links to an external site.<br>\n",
    "         You will follow the different stages of the data pre-processing. This means performing data exploration/visualization, checking for missing values, duplicate removal, and feature selection/reduction.<br>\n",
    "         Once you have downloaded the data, you will prepare a data visualization report. Feel free to provide any additional visualization that might help in better understanding of the data. Write a paragraph about the characteristics of the data you see via visualization.<br>\n",
    "      You must perform N-fold cross-validation of your models and report the mean and standard deviation of any performance metric you use.<br>\n",
    "      Your final model must be implemented and demonstrated as an app using Gradio platform.   <br>\n",
    "      For using Gradio, watch the YouTupe: Building and deploying your first machine learning app in Python using Gradio<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for statistical calculations and data handling\n",
    "import numpy as np # for math operations and data handling\n",
    "import gradio as gr # for user interface demo\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold # K Fold cross validation\n",
    "from sklearn.preprocessing import MinMaxScaler # Feature scaling\n",
    "from sklearn.linear_model import LogisticRegression # Logistical regression classifier\n",
    "from sklearn.ensemble import RandomForestClassifier # Random forest classifier\n",
    "import matplotlib.pyplot as plt # Plotting\n",
    "import seaborn as sns # Cross correlation\n",
    "import time\n",
    "import warnings # Suppressing unwanted warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><h1>Data Loading and Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(\"default of credit card clients.xls\") # read the dataset into a Pandas dataframe\n",
    "\n",
    "# rename the columns using the descriptions for readability, no reason to use the X notation\n",
    "new_column_names = dataset.iloc[0].tolist() # get the column descriptions\n",
    "old_column_names = dataset.columns.tolist() # get the original column names\n",
    "renaming_template = dict(zip(old_column_names, new_column_names)) # match the X_ notated column names to their descriptions\n",
    "renaming_template['X6'] = 'PAY_1' # Fix description (new label) for the first pay column from PAY_0, since the other columns start at 1\n",
    "dataset.rename(columns=renaming_template, inplace = True) # modify the original dataset with the new column names\n",
    "\n",
    "dataset.drop(0, inplace = True) # drop the description row since it is no longer needed\n",
    "\n",
    "dataset.drop(['ID'], axis = 1, inplace = True) # drop the ID row since it just numbers the rows, not needed\n",
    "\n",
    "dataset.drop_duplicates(inplace=True) # dropping the ID column exposed duplicate rows that we don't find before doing that, so drop them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><h1>Visualize Dataset</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_info():\n",
    "    head = dataset.head() # see what the data looks like\n",
    "    description = dataset.describe(include='all')  # get some general info on the data and transpose it\n",
    "    description_labels = description.index.tolist()  # get labels for the description rows\n",
    "    description.insert(0, 'Description', description_labels) # add labels for the descriptions for better visualization\n",
    "\n",
    "    null_counts = dataset.isnull().sum() # check for missing values\n",
    "    null_counts = pd.DataFrame(null_counts).reset_index() # have to make this back into a dataframe for Gradio to be happy\n",
    "    null_counts.columns = ['Column Name', 'Null Count'] # adding some info to make the visualization more readable\n",
    "\n",
    "    duplicate_counts = int(dataset.duplicated().sum()) # check for duplicate rows\n",
    "\n",
    "    class_label_counts = dataset['default payment next month'].value_counts() # get the count of how many defaulted or not defaulted samples are in the dataset\n",
    "    class_label_counts = pd.DataFrame(class_label_counts).reset_index() # have to make this back into a dataframe for Gradio to be happy\n",
    "    class_label_counts['default payment next month'].iloc[0] = \"Not Defaulted\" # make these row labels readable in visualization\n",
    "    class_label_counts['default payment next month'].iloc[1] = \"Defaulted\"\n",
    "\n",
    "    return head, description, null_counts, duplicate_counts, class_label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_corr_heatmap():\n",
    "    columns = ['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', # make a list of all the colums we want to cross correlate\n",
    "                'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\n",
    "                'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "                'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'default payment next month']\n",
    "\n",
    "    columns_to_corr = dataset[columns]  # get the data from the columns we're correlating\n",
    "    corr_matrix = columns_to_corr.corr() # get cross correlations\n",
    "\n",
    "    fig, _ = plt.subplots(figsize = (20, 8))\n",
    "    sns.heatmap(corr_matrix, annot = True, cmap = 'coolwarm', vmin = -1, vmax = 1) # plot correlations, -1 strong negative correlation (blue), 1 strong positive correlation (red)\n",
    "    plt.title('Default of Credit Card Clients Dataset Correlation Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age Vs. Population Defaulted (Negligible Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_age_dist():\n",
    "    defaulted_by_age_counts = dataset.groupby(['AGE', 'default payment next month']).size().unstack(fill_value = 0) # get ages and label of whether each person defaulted\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (12, 5))\n",
    "    defaulted_by_age_counts.plot(kind = 'bar', stacked = True, ax = ax) # plot each age with one bar for those that defaulted and one for not\n",
    "    plt.xlabel('Age')\n",
    "    plt.ylabel('Population')\n",
    "    plt.title('Defaulted Payment by Age (Negligible Correlation)')\n",
    "    plt.legend(title = 'Defaulted', labels = ['No', 'Yes'])\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History of Past Payment (September) Vs. Population Defaulted (Medium-Low Positive Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pay_hist_vs_defaulted():\n",
    "    defaulted_by_age_counts = dataset.groupby(['PAY_1', 'default payment next month']).size().unstack(fill_value = 0) # get first month's pay history and whether the person defaulted\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (12, 5))\n",
    "    defaulted_by_age_counts.plot(kind = 'bar', stacked = True, ax = ax) # plot the population's pay histories with one bar for those that defaulted and one for those that didn't default\n",
    "    plt.xlabel('Months of Payment Delay')\n",
    "    plt.ylabel('Population')\n",
    "    plt.title('History of Past Payment (September) Vs. Population Defaulted (Medium-Low Positive Correlation)')\n",
    "    plt.legend(title = 'Defaulted', labels = ['No', 'Yes'])\n",
    "    plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance Limit Vs. Population Defaulted (Low Negative Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_balance_lim_vs_defaulted():\n",
    "    defaulted_by_age_counts = dataset.groupby(['LIMIT_BAL', 'default payment next month']).size().unstack(fill_value = 0) # get account limit balances and whether the person defaults\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (20, 5))\n",
    "    defaulted_by_age_counts.plot(kind = 'bar', stacked = True, ax = ax) # plot the population's balance limits with one bar for those that defaulted and one for those that didn't\n",
    "    plt.xlabel('Balance Limit')\n",
    "    plt.ylabel('Population')\n",
    "    plt.title('Balance Limit Vs. Population Defaulted (Low Negative Correlation)')\n",
    "    plt.legend(title = 'Defaulted', labels = ['No', 'Yes'])\n",
    "    plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_split():\n",
    "    class_label_counts = dataset['default payment next month'].value_counts() # get the count across the dataset of how many people did or didn't default\n",
    "    class_label_counts = pd.DataFrame(class_label_counts).reset_index() # have to make this back into a dataframe for Gradio to be happy\n",
    "    class_label_counts['default payment next month'].iloc[0] = \"Not Defaulted\" # make these row labels readable in visualization\n",
    "    class_label_counts['default payment next month'].iloc[1] = \"Defaulted\"\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    class_label_counts.plot(kind = 'bar', ax = ax, legend = False)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Population')\n",
    "    plt.title('Dataset Class Label Split')\n",
    "    plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><h1>Logistic Regression Classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_classifier(training_columns):\n",
    "    data = dataset[training_columns] # get the column(s) being used for classification\n",
    "    labels = dataset['default payment next month'].astype(int) # get the labels and convert them to ints\n",
    "\n",
    "    scaler = MinMaxScaler() # create a scaler object that will scale the data between 0 and 1\n",
    "    scaled_data = scaler.fit_transform(data) # scale the data\n",
    "\n",
    "    # set up a K-Fold Cross Validaton object to use 1/5th of the data for testing\n",
    "    # shuffle the data before splitting it\n",
    "    # use a set randomization seed for reproducability\n",
    "    # Stratified K-Fold ensures that the percent split of classes is the same in each fold as there is in the whole dataset\n",
    "    stratified_k_fold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "    model = LogisticRegression(max_iter = 1000, solver='newton-cholesky') # set up a logistics regression object with max epochs to run until convergence before ending, 'newton-cholesky' decreases execution time\n",
    "\n",
    "    # get predictions from cross validation using the lin reg model, min-max scaled data, truth labels, stratified K-Fold object, and return the accuracy for each fold\n",
    "    start_time = time.time()\n",
    "    accuracy = cross_val_score(model, scaled_data, labels, cv = stratified_k_fold, scoring = 'accuracy')\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    mean_accuracy = np.mean(accuracy) # calc average accuracy\n",
    "    accuracy_std_deviation = np.std(accuracy) # get the std dev of the accuracies\n",
    "\n",
    "    scores_2d = [[score] for score in accuracy] # format the results to make them better to add for visualization in gradio\n",
    "    mean_accuracy_str = f\"{mean_accuracy:.4f}\"\n",
    "    std_deviation_str = f\"{accuracy_std_deviation:.4f}\"\n",
    "\n",
    "    return scores_2d, mean_accuracy_str, std_deviation_str, execution_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><h1>Random Forest Classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(training_columns):\n",
    "    data = dataset[training_columns] # get the column(s) being used for classification\n",
    "    labels = dataset['default payment next month'].astype(int) # get the labels and convert them to ints\n",
    "\n",
    "    # set up a K-Fold Cross Validaton object to use 1/5th of the data for testing\n",
    "    # shuffle the data before splitting it\n",
    "    # use a set randomization seed for reproducability\n",
    "    # Stratified K-Fold ensures that the percent split of classes is the same in each fold as there is in the whole dataset\n",
    "    stratified_k_fold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(random_state = 1, max_depth = 1, n_estimators = 1) # create a random forest classifier object with a set seed for reproducability. max_depth and n_estimators being 1 creates a single decision stump, which is optimal performance with PAY_1\n",
    "    \n",
    "    # get predictions from cross validation using the random forest model, data, truth labels, stratified K-Fold object, and return the accuracy for each fold\n",
    "    start_time = time.time()\n",
    "    accuracy = cross_val_score(rf_classifier, data, labels, cv = stratified_k_fold, scoring = 'accuracy')\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracy) # calc average accuracy\n",
    "    accuracy_std_deviation = np.std(accuracy) # get the std dev of the accuracies\n",
    "\n",
    "    scores_2d = [[score] for score in accuracy] # format the results to make them better to add for visualization in gradio\n",
    "    mean_accuracy_str = f\"{mean_accuracy:.4f}\" \n",
    "    std_deviation_str = f\"{accuracy_std_deviation:.4f}\"\n",
    "\n",
    "    return scores_2d, mean_accuracy_str, std_deviation_str, execution_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><h1>Gradio</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Plots Interface Wrapper\n",
    "def visualize_data(): # wrapper function so the gradio interface will plot everything at once for us\n",
    "    heatmap = cross_corr_heatmap() # overall cross correlation heatmap\n",
    "    plot1 = plot_age_dist() # age distribution\n",
    "    plot2 = plot_pay_hist_vs_defaulted() # how on-time payments were at month 1 vs. if the defaulted\n",
    "    plot3 = plot_balance_lim_vs_defaulted() # credit limit vs. if they defaulted\n",
    "    plot4 = plot_class_split() # total population class label split 0 = not defaulted, 1 = defaulted\n",
    "\n",
    "    return heatmap, plot1, plot2, plot3, plot4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "Running on public URL: https://373d74a3d2a6dedb5c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://373d74a3d2a6dedb5c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as credit_default_interface: # make a different tab for every piece of the assignment/application\n",
    "    with gr.Tab(\"Dataset Info\"):\n",
    "        gr.Interface(fn = print_dataset_info, \n",
    "                    inputs = [], \n",
    "                    outputs = [gr.Dataframe(label = \"Example Rows\"),\n",
    "                               gr.Dataframe(label = \"Basic Description Info\"),\n",
    "                               gr.Dataframe(label = \"Missing Values\"),\n",
    "                               gr.Number(label = \"Duplicate Value Count\"),\n",
    "                               gr.Dataframe(label = \"Class Representation\"),])\n",
    "    \n",
    "    with gr.Tab(\"Dataset Plots\"):\n",
    "        gr.Interface(fn = visualize_data, \n",
    "                    inputs = [], \n",
    "                    outputs = [gr.Plot(), \n",
    "                               gr.Plot(), \n",
    "                               gr.Plot(),\n",
    "                               gr.Plot(),\n",
    "                               gr.Plot()])\n",
    "        \n",
    "    with gr.Tab(\"Linear Regression Classification\"):\n",
    "        gr.Interface(fn = lin_reg_classifier, \n",
    "                    inputs = gr.CheckboxGroup(label = \"Select Columns\", choices=dataset.columns.tolist()), \n",
    "                    outputs = [gr.Dataframe(label = \"Fold Accuracies\"),\n",
    "                               gr.Textbox(label = \"Accuracy Mean\"),\n",
    "                               gr.Textbox(label = \"Accuracy Standard Deviation\"),\n",
    "                               gr.Number(label = \"Execution Time (s)\")])\n",
    "        \n",
    "    with gr.Tab(\"Random Forest Classification\"):\n",
    "        gr.Interface(fn = random_forest_classifier, \n",
    "                    inputs = gr.CheckboxGroup(label = \"Select Columns\", choices=dataset.columns.tolist()), \n",
    "                    outputs = [gr.Dataframe(label = \"Fold Accuracies\"),\n",
    "                               gr.Textbox(label = \"Accuracy Mean\"),\n",
    "                               gr.Textbox(label = \"Accuracy Standard Deviation\"),\n",
    "                               gr.Number(label = \"Execution Time (s)\")])\n",
    "\n",
    "credit_default_interface.launch(share = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
